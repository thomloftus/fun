I originally wrote this code in 2008, so my memory is a little fuzzy.

`markov_words` is attempting to train/generate based on sequences of words. I think I wrote that one after I wrote the others which do letter-based training. The probability distribution for any two words is almost always 100% (or _maybe_ 50% in a few rare cases). I think it'd be interesting to see if I ran the word-based training on _much_ larger data sets.

I recall writing this whole thing offline while I was in the car (which is why the corpus is so weird -- I think I copied text off some webpages I had open).
